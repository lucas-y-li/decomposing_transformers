{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers \n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from decompose_bert import BertForMaskedLMDecomposed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25068661805095177"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "verbs = pd.read_csv(\"all_VERBs.csv\")[\"WORD\"]\n",
    "verb_ids = []\n",
    "\n",
    "for verb in verbs.iloc:\n",
    "    i = tokenizer.encode(\" \" + verb, add_special_tokens=False)\n",
    "    if (len(i) == 1):\n",
    "        verb_ids.append(i[0])\n",
    "\n",
    "verb_ids = torch.Tensor(verb_ids).to(int)\n",
    "len(verb_ids) / len(verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_22764/2445828250.py:42: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  number_df = number_df.dropna().reset_index(drop=True).drop(columns=\"token\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject_number</th>\n",
       "      <th>distractor_number</th>\n",
       "      <th>sentence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">verb</th>\n",
       "      <th>verb_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correctness</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>wrong</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cats[MASK]</td>\n",
       "      <td>engages</td>\n",
       "      <td>engage</td>\n",
       "      <td>[24255, 8526]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 8870, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>604</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cats[MASK]</td>\n",
       "      <td>remembers</td>\n",
       "      <td>remember</td>\n",
       "      <td>[17749, 3342]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 8870, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>605</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the chairs[MASK]</td>\n",
       "      <td>observes</td>\n",
       "      <td>observe</td>\n",
       "      <td>[24451, 11949]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 8397, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the dogs[MASK]</td>\n",
       "      <td>encourages</td>\n",
       "      <td>encourage</td>\n",
       "      <td>[16171, 8627]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 6077, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>608</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the trucks[MASK]</td>\n",
       "      <td>avoids</td>\n",
       "      <td>avoid</td>\n",
       "      <td>[26777, 4468]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 9322, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1793</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the cat[MASK]</td>\n",
       "      <td>avoid</td>\n",
       "      <td>avoids</td>\n",
       "      <td>[4468, 26777]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 4937, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1795</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the dog[MASK]</td>\n",
       "      <td>engage</td>\n",
       "      <td>engages</td>\n",
       "      <td>[8526, 24255]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 3899, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1797</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window[MASK]</td>\n",
       "      <td>avoid</td>\n",
       "      <td>avoids</td>\n",
       "      <td>[4468, 26777]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1799</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window[MASK]</td>\n",
       "      <td>engage</td>\n",
       "      <td>engages</td>\n",
       "      <td>[8526, 24255]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1800</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window[MASK]</td>\n",
       "      <td>understand</td>\n",
       "      <td>understands</td>\n",
       "      <td>[3305, 19821]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>561 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id subject_number distractor_number  \\\n",
       "correctness                                          \n",
       "0             603       singular            plural   \n",
       "1             604       singular            plural   \n",
       "2             605       singular            plural   \n",
       "3             606       singular            plural   \n",
       "4             608       singular            plural   \n",
       "..            ...            ...               ...   \n",
       "556          1793         plural          singular   \n",
       "557          1795         plural          singular   \n",
       "558          1797         plural          singular   \n",
       "559          1799         plural          singular   \n",
       "560          1800         plural          singular   \n",
       "\n",
       "                                        sentence         verb                \\\n",
       "correctness                                           correct         wrong   \n",
       "0              The athlete behind the cats[MASK]      engages        engage   \n",
       "1              The athlete behind the cats[MASK]    remembers      remember   \n",
       "2            The athlete behind the chairs[MASK]     observes       observe   \n",
       "3              The athlete behind the dogs[MASK]   encourages     encourage   \n",
       "4            The athlete behind the trucks[MASK]       avoids         avoid   \n",
       "..                                           ...          ...           ...   \n",
       "556                 The women near the cat[MASK]        avoid        avoids   \n",
       "557                 The women near the dog[MASK]       engage       engages   \n",
       "558              The women near the window[MASK]        avoid        avoids   \n",
       "559              The women near the window[MASK]       engage       engages   \n",
       "560              The women near the window[MASK]   understand   understands   \n",
       "\n",
       "                verb_tokens                                    sentence_tokens  \n",
       "correctness                                                                     \n",
       "0             [24255, 8526]  [101, 1996, 8258, 2369, 1996, 8870, 103, 102, ...  \n",
       "1             [17749, 3342]  [101, 1996, 8258, 2369, 1996, 8870, 103, 102, ...  \n",
       "2            [24451, 11949]  [101, 1996, 8258, 2369, 1996, 8397, 103, 102, ...  \n",
       "3             [16171, 8627]  [101, 1996, 8258, 2369, 1996, 6077, 103, 102, ...  \n",
       "4             [26777, 4468]  [101, 1996, 8258, 2369, 1996, 9322, 103, 102, ...  \n",
       "..                      ...                                                ...  \n",
       "556           [4468, 26777]  [101, 1996, 2308, 2379, 1996, 4937, 103, 102, ...  \n",
       "557           [8526, 24255]  [101, 1996, 2308, 2379, 1996, 3899, 103, 102, ...  \n",
       "558           [4468, 26777]  [101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...  \n",
       "559           [8526, 24255]  [101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...  \n",
       "560           [3305, 19821]  [101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...  \n",
       "\n",
       "[561 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_df = pd.read_csv(\"nounpp.tsv\", delimiter=\"\\t\")\n",
    "max_length = 16\n",
    "\n",
    "number_df[\"id\"] = number_df[\"id\"].apply(lambda x: int(x[2:]))\n",
    "\n",
    "# only keep sentences that are plural/singular or singular/plural (distractor has different number)\n",
    "number_df[\"subject_distractor_number\"] = number_df[\"subject_distractor_number\"].apply(\n",
    "    lambda x: x if x == \"singular_plural\" or x == \"plural_singular\" else np.nan\n",
    ")\n",
    "number_df.dropna(inplace=True)\n",
    "\n",
    "number_df[\"subject_number\"] = number_df[\"subject_distractor_number\"].apply(lambda x: x.split(\"_\")[0])\n",
    "number_df[\"distractor_number\"] = number_df[\"subject_distractor_number\"].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "number_df[\"verb\"] = number_df[\"sentence\"].apply(lambda x: \" \" + x.split(\" \")[-1])\n",
    "number_df[\"sentence\"] = number_df[\"sentence\"].apply(lambda x: \" \".join(x.split(\" \")[:-1]) + \"[MASK]\")\n",
    "\n",
    "number_df = number_df.drop(\n",
    "    columns=[\"subject_distractor_number\"]\n",
    "    ).pivot(index=[\"id\", \"subject_number\", \"distractor_number\", \"sentence\"], columns=[\"correctness\"], values=[\"verb\"]).reset_index()\n",
    "\n",
    "\n",
    "def get_token(correct_token, wrong_token):\n",
    "    # drop if longer than 1 token\n",
    "    if len(correct_token) > 3 or len(wrong_token) > 3:\n",
    "        return np.nan\n",
    "\n",
    "    return [correct_token[1], wrong_token[1]]\n",
    "\n",
    "number_df[(\"token\", \"correct\")] = tokenizer(number_df[(\"verb\", \"correct\")].to_list())[\"input_ids\"]\n",
    "number_df[(\"token\", \"wrong\")] = tokenizer(number_df[(\"verb\", \"wrong\")].to_list())[\"input_ids\"]\n",
    "\n",
    "number_df[(\"verb_tokens\")] = number_df.apply(\n",
    "    lambda x: get_token(x[(\"token\", \"correct\")], x[(\"token\", \"wrong\")]),\n",
    "    axis=1)\n",
    "\n",
    "number_df[(\"sentence_tokens\")] = tokenizer(number_df[(\"sentence\", \"\")].to_list(),\n",
    "                                           max_length=max_length,\n",
    "                                           padding=\"max_length\")[\"input_ids\"]\n",
    "\n",
    "# drop duplicates\n",
    "number_df = number_df.dropna().reset_index(drop=True).drop(columns=\"token\")\n",
    "number_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1996, 8258, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 8258, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2879, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 7500, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 7500, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 7500, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2269, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2767, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2767, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2611, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 4845, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5160, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2158, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2158, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2388, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2388, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 3220, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 3220, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 3836, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 4470, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 6778, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2450, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2450, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2369, 1996, 2482, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2369, 1996, 4937, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2369, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2369, 1996, 3332, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 3875, 1996, 7997, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 3875, 1996, 4624, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 3875, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 3875, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3392, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3392, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 4744, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3332, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2369, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2369, 1996, 3899, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 7997, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 2482, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 4744, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 3332, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 4624, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 4624, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 4624, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 3392, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_component_masks(sentence_tokens):\n",
    "    # prep = [639, 13276, 583] # beside, near, behind\n",
    "    prep = tokenizer.encode(' behind beside near')[1:-1]\n",
    "\n",
    "    # always starts with 0, 133\n",
    "    i = 2\n",
    "    init_i = 0\n",
    "    The_i = 1\n",
    "    subj_i = []\n",
    "\n",
    "    while sentence_tokens[i] not in prep:\n",
    "        subj_i.append(i)\n",
    "        i += 1\n",
    "        \n",
    "    prep_i = i\n",
    "    the_i = i + 1\n",
    "    distractor_id = []\n",
    "    i += 2\n",
    "\n",
    "    while sentence_tokens[i] != tokenizer.mask_token_id:\n",
    "        distractor_id.append(i)\n",
    "        i += 1\n",
    "    \n",
    "    mask_i = i\n",
    "    \n",
    "    # place each one in a separate array \n",
    "    component_masks = np.zeros((6, max_length))\n",
    "    for n, component in enumerate([\n",
    "        init_i, The_i, subj_i, prep_i, the_i, distractor_id\n",
    "    ]):\n",
    "        component_masks[n, component] = 1\n",
    "    \n",
    "    if mask_i != 6:\n",
    "        print(sentence_tokens)\n",
    "        print(component_masks)\n",
    "        print(mask_i)\n",
    "        print()\n",
    "    return component_masks, mask_i\n",
    "\n",
    "\n",
    "number_df[\"beta_mask\"] = number_df[\"sentence_tokens\"].apply(make_component_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportion_contribution(n, decomposed_model):\n",
    "    beta_masks = torch.tensor(number_df[\"beta_mask\"][n][0])\n",
    "    verb_tokens = torch.tensor(number_df[\"verb_tokens\"][n])\n",
    "    inputs = torch.tensor(number_df[\"sentence_tokens\"][n]).unsqueeze(0)\n",
    "    mask_i = number_df[\"beta_mask\"][n][1]\n",
    "\n",
    "    contribution_logits = torch.zeros((6, 2))\n",
    "    # print(tokenizer.convert_ids_to_tokens(verb_tokens))\n",
    "\n",
    "    for i, mask in enumerate(beta_masks):\n",
    "        beta_mask = torch.stack([mask, 1 - mask]).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            result = decomposed_model(input_ids=inputs,\n",
    "                            beta_mask=beta_mask)\n",
    "        result = result[0, 0, mask_i, :]\n",
    "        \n",
    "\n",
    "        # normalize across all verbs\n",
    "        result = result - result[verb_ids].mean()\n",
    "        result = result / result[verb_ids].std()\n",
    "\n",
    "        # logits = result[verb_tokens]\n",
    "        # logits = result\n",
    "        # beta_z_t / z_t\n",
    "        contribution = result[verb_tokens]\n",
    "        contribution_logits[i, :] = contribution\n",
    "\n",
    "    return contribution_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCD w/ fixed bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "decomposed_model = BertForMaskedLMDecomposed.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    debug=False,\n",
    "    shapley_include_bias=False,\n",
    "    generalized=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "contribution_logits_sp = []\n",
    "contribution_logits_ps = []\n",
    "\n",
    "for i in range(len(number_df)):\n",
    "    if number_df[\"subject_number\"][i] == \"singular\":\n",
    "        contribution_logits_sp.append(get_proportion_contribution(i, decomposed_model))\n",
    "    else:\n",
    "        contribution_logits_ps.append(get_proportion_contribution(i, decomposed_model))\n",
    "    if i % 20 == 0:\n",
    "        print(i)\n",
    "\n",
    "contribution_logits_sp = torch.stack(contribution_logits_sp)\n",
    "contribution_logits_ps = torch.stack(contribution_logits_ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5677, -0.0718],\n",
       "        [ 1.0677, -0.7764],\n",
       "        [ 0.9124, -0.4551],\n",
       "        [ 0.1957, -0.5266],\n",
       "        [ 0.0553, -0.4168],\n",
       "        [-0.4131,  0.5885]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_logits_sp.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0424,  0.5418],\n",
       "        [-0.3427,  0.3625],\n",
       "        [ 1.2473, -1.1303],\n",
       "        [-0.4833, -0.1004],\n",
       "        [-0.5987,  0.0482],\n",
       "        [-0.5911,  0.2511]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_logits_ps.mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CD w/ fixed bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "decomposed_model = BertForMaskedLMDecomposed.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    debug=False,\n",
    "    shapley_include_bias=False,\n",
    "    generalized=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "contribution_logits_sp = []\n",
    "contribution_logits_ps = []\n",
    "\n",
    "for i in range(len(number_df)):\n",
    "    if number_df[\"subject_number\"][i] == \"singular\":\n",
    "        contribution_logits_sp.append(get_proportion_contribution(i, decomposed_model))\n",
    "    else:\n",
    "        contribution_logits_ps.append(get_proportion_contribution(i,decomposed_model))\n",
    "    if i % 20 == 0:\n",
    "        print(i)\n",
    "\n",
    "contribution_logits_sp = torch.stack(contribution_logits_sp)\n",
    "contribution_logits_ps = torch.stack(contribution_logits_ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0549, -0.1623],\n",
       "        [-0.1417, -0.2288],\n",
       "        [-0.1421, -0.1209],\n",
       "        [-0.2537, -0.1571],\n",
       "        [-0.0379, -0.0035],\n",
       "        [ 0.2645,  0.0168]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_logits_sp.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1887,  0.1980],\n",
       "        [-0.2756, -0.0090],\n",
       "        [ 0.0306, -0.1814],\n",
       "        [-0.0122, -0.1717],\n",
       "        [ 0.1294,  0.0916],\n",
       "        [ 0.0891,  0.1201]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_logits_ps.mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCD w/ permuted bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "decomposed_model = BertForMaskedLMDecomposed.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    debug=False,\n",
    "    shapley_include_bias=True,\n",
    "    generalized=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "contribution_logits_sp = []\n",
    "contribution_logits_ps = []\n",
    "\n",
    "for i in range(len(number_df)):\n",
    "    if number_df[\"subject_number\"][i] == \"singular\":\n",
    "        contribution_logits_sp.append(get_proportion_contribution(i, decomposed_model))\n",
    "    else:\n",
    "        contribution_logits_ps.append(get_proportion_contribution(i, decomposed_model))\n",
    "    if i % 20 == 0:\n",
    "        print(i)\n",
    "\n",
    "contribution_logits_sp = torch.stack(contribution_logits_sp)\n",
    "contribution_logits_ps = torch.stack(contribution_logits_ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0223,  0.0125],\n",
       "        [ 0.2100, -0.4557],\n",
       "        [ 0.1324, -0.3538],\n",
       "        [ 0.2448, -0.4292],\n",
       "        [ 0.1691, -0.5008],\n",
       "        [ 0.1831, -0.4290]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_logits_sp.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1756, -0.1530],\n",
       "        [-0.3966,  0.3387],\n",
       "        [-0.3167,  0.2745],\n",
       "        [-0.4046,  0.3177],\n",
       "        [-0.4401,  0.2723],\n",
       "        [-0.3792,  0.3046]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_logits_ps.mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CD w/ permuted bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "decomposed_model = BertForMaskedLMDecomposed.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    debug=False,\n",
    "    shapley_include_bias=True,\n",
    "    generalized=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "contribution_logits_sp = []\n",
    "contribution_logits_ps = []\n",
    "\n",
    "for i in range(len(number_df)):\n",
    "    if number_df[\"subject_number\"][i] == \"singular\":\n",
    "        contribution_logits_sp.append(get_proportion_contribution(i, decomposed_model))\n",
    "    else:\n",
    "        contribution_logits_ps.append(get_proportion_contribution(i, decomposed_model))\n",
    "    if i % 20 == 0:\n",
    "        print(i)\n",
    "\n",
    "contribution_logits_sp = torch.stack(contribution_logits_sp)\n",
    "contribution_logits_ps = torch.stack(contribution_logits_ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1111,  0.0112],\n",
       "        [-0.7784, -0.1337],\n",
       "        [ 0.2989,  0.0441],\n",
       "        [-0.8928,  0.0042],\n",
       "        [ 0.4510,  0.2086],\n",
       "        [-0.3137,  0.0163]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_logits_sp.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0023, -1.0468],\n",
       "        [-0.2214, -0.9572],\n",
       "        [ 0.0493,  0.4802],\n",
       "        [-0.0234, -0.9137],\n",
       "        [-0.0327,  0.1585],\n",
       "        [-0.1561, -0.2880]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_logits_ps.mean(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaphor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
