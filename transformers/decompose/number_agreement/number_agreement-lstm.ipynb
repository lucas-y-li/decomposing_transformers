{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbdc8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from decompose_lstm import DecomposedLSTM\n",
    "from lstm_dictionary import Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd54912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../lstm/data/model/state_dict.pt\"\n",
    "vocab_path = \"../../lstm/data/model/vocab.txt\"\n",
    "\n",
    "decomposed_model = DecomposedLSTM.from_pretrained(model_path, shapley_include_bias=False)\n",
    "tokenizer = Dictionary(vocab_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e795b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject_number</th>\n",
       "      <th>distractor_number</th>\n",
       "      <th>sentence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">verb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correctness</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the bikes</td>\n",
       "      <td>approves</td>\n",
       "      <td>approve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cars</td>\n",
       "      <td>inspires</td>\n",
       "      <td>inspire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cats</td>\n",
       "      <td>engages</td>\n",
       "      <td>engage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cats</td>\n",
       "      <td>remembers</td>\n",
       "      <td>remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>605</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the chairs</td>\n",
       "      <td>observes</td>\n",
       "      <td>observe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1796</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the tree</td>\n",
       "      <td>greet</td>\n",
       "      <td>greets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1797</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window</td>\n",
       "      <td>avoid</td>\n",
       "      <td>avoids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1798</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window</td>\n",
       "      <td>discourage</td>\n",
       "      <td>discourages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1799</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window</td>\n",
       "      <td>engage</td>\n",
       "      <td>engages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1800</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window</td>\n",
       "      <td>understand</td>\n",
       "      <td>understands</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id subject_number distractor_number  \\\n",
       "correctness                                          \n",
       "0             601       singular            plural   \n",
       "1             602       singular            plural   \n",
       "2             603       singular            plural   \n",
       "3             604       singular            plural   \n",
       "4             605       singular            plural   \n",
       "...           ...            ...               ...   \n",
       "1195         1796         plural          singular   \n",
       "1196         1797         plural          singular   \n",
       "1197         1798         plural          singular   \n",
       "1198         1799         plural          singular   \n",
       "1199         1800         plural          singular   \n",
       "\n",
       "                                  sentence         verb                \n",
       "correctness                                     correct         wrong  \n",
       "0             The athlete behind the bikes     approves       approve  \n",
       "1              The athlete behind the cars     inspires       inspire  \n",
       "2              The athlete behind the cats      engages        engage  \n",
       "3              The athlete behind the cats    remembers      remember  \n",
       "4            The athlete behind the chairs     observes       observe  \n",
       "...                                    ...          ...           ...  \n",
       "1195               The women near the tree        greet        greets  \n",
       "1196             The women near the window        avoid        avoids  \n",
       "1197             The women near the window   discourage   discourages  \n",
       "1198             The women near the window       engage       engages  \n",
       "1199             The women near the window   understand   understands  \n",
       "\n",
       "[1200 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_df = pd.read_csv(\"nounpp.tsv\", delimiter=\"\\t\")\n",
    "max_length = 16\n",
    "\n",
    "number_df[\"id\"] = number_df[\"id\"].apply(lambda x: int(x[2:]))\n",
    "\n",
    "# only keep sentences that are plural/singular or singular/plural (distractor has different number)\n",
    "number_df[\"subject_distractor_number\"] = number_df[\"subject_distractor_number\"].apply(\n",
    "    lambda x: x if x == \"singular_plural\" or x == \"plural_singular\" else np.nan\n",
    ")\n",
    "number_df.dropna(inplace=True)\n",
    "\n",
    "number_df[\"subject_number\"] = number_df[\"subject_distractor_number\"].apply(lambda x: x.split(\"_\")[0])\n",
    "number_df[\"distractor_number\"] = number_df[\"subject_distractor_number\"].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "number_df[\"verb\"] = number_df[\"sentence\"].apply(lambda x: \" \" + x.split(\" \")[-1])\n",
    "number_df[\"sentence\"] = number_df[\"sentence\"].apply(lambda x: \" \".join(x.split(\" \")[:-1]))\n",
    "\n",
    "number_df = number_df.drop(\n",
    "    columns=[\"subject_distractor_number\"]\n",
    "    ).pivot(index=[\"id\", \"subject_number\", \"distractor_number\", \"sentence\"], columns=[\"correctness\"], values=[\"verb\"]).reset_index()\n",
    "\n",
    "number_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a336e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject_number</th>\n",
       "      <th>distractor_number</th>\n",
       "      <th>sentence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">verb</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>correct_token</th>\n",
       "      <th>wrong_token</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correctness</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>wrong</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the bikes</td>\n",
       "      <td>approves</td>\n",
       "      <td>approve</td>\n",
       "      <td>[tensor(146), tensor(21749), tensor(2230), ten...</td>\n",
       "      <td>42696</td>\n",
       "      <td>11336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cars</td>\n",
       "      <td>inspires</td>\n",
       "      <td>inspire</td>\n",
       "      <td>[tensor(146), tensor(21749), tensor(2230), ten...</td>\n",
       "      <td>33476</td>\n",
       "      <td>17370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cats</td>\n",
       "      <td>engages</td>\n",
       "      <td>engage</td>\n",
       "      <td>[tensor(146), tensor(21749), tensor(2230), ten...</td>\n",
       "      <td>18223</td>\n",
       "      <td>3610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cats</td>\n",
       "      <td>remembers</td>\n",
       "      <td>remember</td>\n",
       "      <td>[tensor(146), tensor(21749), tensor(2230), ten...</td>\n",
       "      <td>8671</td>\n",
       "      <td>2030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>605</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the chairs</td>\n",
       "      <td>observes</td>\n",
       "      <td>observe</td>\n",
       "      <td>[tensor(146), tensor(21749), tensor(2230), ten...</td>\n",
       "      <td>8739</td>\n",
       "      <td>8501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id subject_number distractor_number  \\\n",
       "correctness                                         \n",
       "0            601       singular            plural   \n",
       "1            602       singular            plural   \n",
       "2            603       singular            plural   \n",
       "3            604       singular            plural   \n",
       "4            605       singular            plural   \n",
       "\n",
       "                                  sentence        verb             \\\n",
       "correctness                                    correct      wrong   \n",
       "0             The athlete behind the bikes    approves    approve   \n",
       "1              The athlete behind the cars    inspires    inspire   \n",
       "2              The athlete behind the cats     engages     engage   \n",
       "3              The athlete behind the cats   remembers   remember   \n",
       "4            The athlete behind the chairs    observes    observe   \n",
       "\n",
       "                                               sentence_tokens correct_token  \\\n",
       "correctness                                                                    \n",
       "0            [tensor(146), tensor(21749), tensor(2230), ten...         42696   \n",
       "1            [tensor(146), tensor(21749), tensor(2230), ten...         33476   \n",
       "2            [tensor(146), tensor(21749), tensor(2230), ten...         18223   \n",
       "3            [tensor(146), tensor(21749), tensor(2230), ten...          8671   \n",
       "4            [tensor(146), tensor(21749), tensor(2230), ten...          8739   \n",
       "\n",
       "            wrong_token  \n",
       "correctness              \n",
       "0                 11336  \n",
       "1                 17370  \n",
       "2                  3610  \n",
       "3                  2030  \n",
       "4                  8501  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_df[\"sentence_tokens\"] = number_df[\"sentence\"].apply(tokenizer.tokenize)\n",
    "number_df[\"correct_token\"] = number_df[(\"verb\", \"correct\")].apply(lambda x: tokenizer.tokenize(x).item())\n",
    "number_df[\"wrong_token\"] = number_df[(\"verb\", \"wrong\")].apply(lambda x: tokenizer.tokenize(x).item())\n",
    "\n",
    "number_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = 5\n",
    "assert number_df[\"sentence_tokens\"].apply(len).unique() == [sentence_len]\n",
    "\n",
    "\n",
    "def split_pos_neg_contributions(logits):\n",
    "    \"\"\"\n",
    "    shape: (num_contributions + 1 (bias), num_classes)\n",
    "    \"\"\"\n",
    "   # put negative \"positive\" contributions in the negative category\n",
    "    # put negative \"negative\" contributions in the positive category\n",
    "    positive_mask = (logits > 0).astype(int)\n",
    "\n",
    "    positive_logits = logits[..., 1] * positive_mask[..., 1]\n",
    "    positive_logits -= logits[..., 0] * (1 - positive_mask[..., 0])\n",
    "\n",
    "    negative_logits = logits[..., 0] * positive_mask[..., 0]\n",
    "    negative_logits -= logits[..., 1] * (1 - positive_mask[..., 1])\n",
    "\n",
    "    # assert (np.all(positive_logits >= 0))\n",
    "    # assert (np.all(negative_logits >= 0))\n",
    "\n",
    "    binary_logits = np.stack(\n",
    "        [negative_logits, positive_logits], axis=-2\n",
    "    )\n",
    "    # print(binary_logits.shape)\n",
    "    return binary_logits\n",
    "\n",
    "\n",
    "def get_jumulet_contribution(sentence_tokens, correct_token, wrong_token):\n",
    "    all_contributions = torch.empty((sentence_len + 1,2))\n",
    "\n",
    "    for i in range(-1, sentence_len):\n",
    "        beta_mask = torch.zeros(sentence_len,)\n",
    "        init_in_beta = False\n",
    "        if i == -1:\n",
    "            # get contribution of the initial state\n",
    "            init_in_beta = True\n",
    "        else:\n",
    "            # get contribution of the token at index i\n",
    "            beta_mask[i] = 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            beta, gamma, bias = decomposed_model(\n",
    "                sentence_tokens.unsqueeze(0),\n",
    "                beta_mask.unsqueeze(0),\n",
    "                init_in_beta=init_in_beta\n",
    "                )\n",
    "            # get last hidden states\n",
    "            z = beta + gamma + bias\n",
    "            contribution = beta / z\n",
    "            c = contribution[0, -1, (correct_token, wrong_token)]\n",
    "            \n",
    "            all_contributions[i + 1] = c\n",
    "    return all_contributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7fdfb",
   "metadata": {},
   "source": [
    "### GCD w/ fixed bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1fe2722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2439,  0.2071,  0.9643,  ..., -1.9202, -1.9483,  0.3526],\n",
      "         [ 0.9491,  5.1321,  9.4497,  ..., -2.9997, -2.5698, -0.6521],\n",
      "         [ 0.2316, -1.0468,  8.4410,  ..., -2.3390, -3.8668,  0.1466],\n",
      "         [-2.9794,  1.0338,  3.7435,  ..., -2.9436, -4.0727, -0.6532],\n",
      "         [ 1.0138,  6.9565, 11.0821,  ..., -2.7540, -3.0840, -1.4964]]])\n",
      "tensor([[[ 0.2439,  0.2071,  0.9643,  ..., -1.9202, -1.9483,  0.3526],\n",
      "         [ 0.9491,  5.1321,  9.4497,  ..., -2.9997, -2.5698, -0.6521],\n",
      "         [ 0.2316, -1.0468,  8.4410,  ..., -2.3390, -3.8668,  0.1466],\n",
      "         [-2.9794,  1.0338,  3.7435,  ..., -2.9436, -4.0727, -0.6532],\n",
      "         [ 1.0138,  6.9565, 11.0821,  ..., -2.7540, -3.0840, -1.4964]]])\n",
      "tensor([[[ 0.2439,  0.2071,  0.9643,  ..., -1.9202, -1.9483,  0.3526],\n",
      "         [ 0.9491,  5.1321,  9.4497,  ..., -2.9997, -2.5698, -0.6521],\n",
      "         [ 0.2316, -1.0468,  8.4410,  ..., -2.3390, -3.8668,  0.1466],\n",
      "         [-2.9794,  1.0338,  3.7435,  ..., -2.9436, -4.0727, -0.6532],\n",
      "         [ 1.0138,  6.9565, 11.0821,  ..., -2.7540, -3.0840, -1.4964]]])\n",
      "tensor([[[ 0.2439,  0.2071,  0.9643,  ..., -1.9202, -1.9483,  0.3526],\n",
      "         [ 0.9491,  5.1321,  9.4497,  ..., -2.9997, -2.5698, -0.6521],\n",
      "         [ 0.2316, -1.0468,  8.4410,  ..., -2.3390, -3.8668,  0.1466],\n",
      "         [-2.9794,  1.0338,  3.7435,  ..., -2.9436, -4.0727, -0.6532],\n",
      "         [ 1.0138,  6.9565, 11.0821,  ..., -2.7540, -3.0840, -1.4964]]])\n",
      "tensor([[[ 0.2439,  0.2071,  0.9643,  ..., -1.9202, -1.9483,  0.3526],\n",
      "         [ 0.9491,  5.1321,  9.4497,  ..., -2.9997, -2.5698, -0.6521],\n",
      "         [ 0.2316, -1.0468,  8.4410,  ..., -2.3390, -3.8668,  0.1466],\n",
      "         [-2.9794,  1.0338,  3.7435,  ..., -2.9436, -4.0727, -0.6532],\n",
      "         [ 1.0138,  6.9565, 11.0821,  ..., -2.7540, -3.0840, -1.4964]]])\n",
      "tensor([[[ 0.2439,  0.2071,  0.9643,  ..., -1.9202, -1.9483,  0.3526],\n",
      "         [ 0.9491,  5.1321,  9.4497,  ..., -2.9997, -2.5698, -0.6521],\n",
      "         [ 0.2316, -1.0468,  8.4410,  ..., -2.3390, -3.8668,  0.1466],\n",
      "         [-2.9794,  1.0338,  3.7435,  ..., -2.9436, -4.0727, -0.6532],\n",
      "         [ 1.0138,  6.9565, 11.0821,  ..., -2.7540, -3.0840, -1.4964]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7557,  0.4462],\n",
       "        [ 0.3445,  0.3268],\n",
       "        [ 0.6733,  0.2016],\n",
       "        [-0.4387, -0.3993],\n",
       "        [ 0.0400, -0.0152],\n",
       "        [ 0.2623,  0.8895]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_jumulet_contribution(\n",
    "    torch.tensor([146, 21749,  2230,     3, 14159]),\n",
    "    42696,\n",
    "    11336\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cac5646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 146, 6022,  220,    3, 1041, 4814])\n",
      "tensor([ 146, 6022,  220,    3, 1041, 2678])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"The doctor near the dogs knows\"))\n",
    "print(tokenizer.tokenize(\"The doctor near the dogs know\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d909aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2786,  0.1383],\n",
       "        [ 0.0915,  0.0613],\n",
       "        [ 0.3559,  0.2025],\n",
       "        [-0.0963,  0.0339],\n",
       "        [ 0.0789, -0.0953],\n",
       "        [ 0.2852,  0.4062]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_jumulet_contribution(\n",
    "    torch.tensor([ 146, 6022,  220,    3, 1041]),\n",
    "    4814,\n",
    "    2678\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bbb776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/4009221104.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"sentence_tokens\"][0],\n",
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/4009221104.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"correct_token\"][0],\n",
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/4009221104.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"wrong_token\"][0]\n"
     ]
    }
   ],
   "source": [
    "number_df[\"jumulet_contribution_fixed\"] = number_df.apply(\n",
    "    lambda x: get_jumulet_contribution(\n",
    "        x[\"sentence_tokens\"][0],\n",
    "        x[\"correct_token\"][0],\n",
    "        x[\"wrong_token\"][0]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5216e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# singular subject, plural distractor\n",
    "sp_contributions = np.stack(\n",
    "    number_df[number_df[\"distractor_number\"] == \"plural\"][\"jumulet_contribution_fixed\"]\n",
    "    )\n",
    "\n",
    "# singular subject\n",
    "ps_contributions = np.stack(\n",
    "    number_df[number_df[\"distractor_number\"] == \"singular\"][\"jumulet_contribution_fixed\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95753546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43970898,  0.4404421 ],\n",
       "       [ 0.23971453,  0.17095037],\n",
       "       [ 0.20777053, -0.2277045 ],\n",
       "       [-0.19155994,  0.07164335],\n",
       "       [ 0.09507721, -0.00318193],\n",
       "       [ 0.44742966,  0.79140973]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_contributions.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560ab0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24841301,  1.0414895 ],\n",
       "       [ 0.13884118,  0.43080363],\n",
       "       [ 0.38182613, -0.8785911 ],\n",
       "       [ 0.06271271, -0.35090494],\n",
       "       [ 0.08335493,  0.49208227],\n",
       "       [ 0.18639894,  0.98074573]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_contributions.mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46b8b6",
   "metadata": {},
   "source": [
    "### GCD w/ shapely bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_model.shapley_include_bias = True\n",
    "decomposed_model.generalized = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53588fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # activations for \". <eos>\"\n",
    "    init_phrase = torch.LongTensor([18, 19]).unsqueeze(0)\n",
    "    embed = decomposed_model.model.encoder(init_phrase)\n",
    "    _, hidden = decomposed_model.model.rnn(embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a055d024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 1 doctor\n",
      "tensor(0.2253, dtype=torch.float64)\n",
      "-1 2 near\n",
      "tensor(0.1425, dtype=torch.float64)\n",
      "-1 3 the\n",
      "tensor(-0.0316, dtype=torch.float64)\n",
      "-1 4 dogs\n",
      "tensor(0.5632, dtype=torch.float64)\n",
      "0 1 doctor\n",
      "tensor(0.4304, dtype=torch.float64)\n",
      "0 2 near\n",
      "tensor(0.1263, dtype=torch.float64)\n",
      "0 3 the\n",
      "tensor(-0.0108, dtype=torch.float64)\n",
      "0 4 dogs\n",
      "tensor(0.4810, dtype=torch.float64)\n",
      "1 2 near\n",
      "tensor(-0.0610, dtype=torch.float64)\n",
      "1 3 the\n",
      "tensor(0.0038, dtype=torch.float64)\n",
      "1 4 dogs\n",
      "tensor(0.6334, dtype=torch.float64)\n",
      "2 3 the\n",
      "tensor(0.1513, dtype=torch.float64)\n",
      "2 4 dogs\n",
      "tensor(0.0379, dtype=torch.float64)\n",
      "3 4 dogs\n",
      "tensor(0.0574, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "all_contributions = torch.empty((sentence_len + 1,2))\n",
    "sentence_tokens = torch.tensor([ 146, 6022,  220,    3, 1041])\n",
    "contributions = torch.zeros((6,6))\n",
    "correct_token = 4814\n",
    "wrong_token = 2678\n",
    "\n",
    "for i in range(-1, sentence_len):\n",
    "    beta_mask = torch.zeros(sentence_len,)\n",
    "    init_in_beta = False\n",
    "    if i == -1:\n",
    "        # get contribution of the initial state\n",
    "        init_in_beta = True\n",
    "    else:\n",
    "        # get contribution of the token at index i\n",
    "        beta_mask[i] = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        beta, gamma, bias = decomposed_model(\n",
    "            sentence_tokens.unsqueeze(0),\n",
    "            beta_mask.unsqueeze(0),\n",
    "            init_in_beta=init_in_beta\n",
    "            )\n",
    "        z, _ = decomposed_model.model.forward(\n",
    "            sentence_tokens.unsqueeze(0),\n",
    "            hidden\n",
    "        )\n",
    "        # get last hidden states\n",
    "        # z = beta + gamma + bias\n",
    "\n",
    "        contribution = beta / z\n",
    "        for j in range(max(i+1, 1), sentence_len):\n",
    "            future_token = sentence_tokens[j].item()\n",
    "            print(i, j, tokenizer.idx2word[future_token])\n",
    "            print(contribution[0, j - 1, future_token])\n",
    "            contributions[i + 1, j - 1] = contribution[0, j - 1, future_token]\n",
    "        \n",
    "        contributions[i+1, 4] = contribution[0, -1, correct_token]\n",
    "        contributions[i+1, 5] = contribution[0, -1, wrong_token]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "985602d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23,  0.14, -0.03,  0.56,  0.49,  0.  ],\n",
       "       [ 0.43,  0.13, -0.01,  0.48,  0.34,  0.07],\n",
       "       [ 0.  , -0.06,  0.  ,  0.63,  0.53,  0.24],\n",
       "       [ 0.  ,  0.  ,  0.15,  0.04,  0.1 ,  0.05],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.06,  0.14, -0.29],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.33,  0.29]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contributions.numpy().round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98b0e2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4932,  0.0014],\n",
       "        [ 0.3426,  0.0726],\n",
       "        [ 0.5288,  0.2434],\n",
       "        [ 0.0958,  0.0529],\n",
       "        [ 0.1420, -0.2898],\n",
       "        [ 0.3310,  0.2850]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_jumulet_contribution(\n",
    "    torch.tensor([ 146, 6022,  220,    3, 1041]),\n",
    "    4814,\n",
    "    2678\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc917d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/2760458623.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"sentence_tokens\"][0],\n",
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/2760458623.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"correct_token\"][0],\n",
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/2760458623.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"wrong_token\"][0]\n"
     ]
    }
   ],
   "source": [
    "number_df[\"jumulet_contribution_all\"] = number_df.apply(\n",
    "    lambda x: get_jumulet_contribution(\n",
    "        x[\"sentence_tokens\"][0],\n",
    "        x[\"correct_token\"][0],\n",
    "        x[\"wrong_token\"][0]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88428785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# singular subject, plural distractor\n",
    "sp_contributions = np.stack(\n",
    "    number_df[number_df[\"distractor_number\"] == \"plural\"][\"jumulet_contribution_all\"]\n",
    "    )\n",
    "\n",
    "# singular subject\n",
    "ps_contributions = np.stack(\n",
    "    number_df[number_df[\"distractor_number\"] == \"singular\"][\"jumulet_contribution_all\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfc01b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7526021 ,  0.16763318],\n",
       "       [ 0.50965893,  0.16434468],\n",
       "       [ 0.48675588, -0.11615125],\n",
       "       [ 0.03766717,  0.06272028],\n",
       "       [ 0.24761301, -0.26764938],\n",
       "       [ 0.55568933,  0.4488798 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_contributions.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1104aff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12187713,  1.8369094 ],\n",
       "       [ 0.04983244,  1.3322237 ],\n",
       "       [ 0.5689856 , -0.1285119 ],\n",
       "       [ 0.04693957,  0.23761767],\n",
       "       [-0.08282404,  0.7737088 ],\n",
       "       [-0.04266999,  1.1255189 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_contributions.mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a881ed",
   "metadata": {},
   "source": [
    "### CD w/ fixed bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3401eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_model.shapley_include_bias = False\n",
    "decomposed_model.generalized = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8fc6af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.7735e-02,  1.6436e-02],\n",
       "        [ 2.6688e-02,  2.5698e-02],\n",
       "        [ 6.7182e-02,  1.2396e-01],\n",
       "        [-6.5648e-05,  1.6064e-01],\n",
       "        [ 1.2273e-01,  1.0233e-01],\n",
       "        [ 1.7873e-01,  4.8859e-01]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_jumulet_contribution(\n",
    "    torch.tensor([ 146, 6022,  220,    3, 1041]),\n",
    "    4814,\n",
    "    2678\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5836a308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/2305937806.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"sentence_tokens\"][0],\n",
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/2305937806.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"correct_token\"][0],\n",
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/2305937806.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"wrong_token\"][0]\n"
     ]
    }
   ],
   "source": [
    "number_df[\"murdoch_contribution_all\"] = number_df.apply(\n",
    "    lambda x: get_jumulet_contribution(\n",
    "        x[\"sentence_tokens\"][0],\n",
    "        x[\"correct_token\"][0],\n",
    "        x[\"wrong_token\"][0]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e36cf901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# singular subject, plural distractor\n",
    "sp_contributions = np.stack(\n",
    "    number_df[number_df[\"distractor_number\"] == \"plural\"][\"murdoch_contribution_all\"]\n",
    "    )\n",
    "\n",
    "# singular subject\n",
    "ps_contributions = np.stack(\n",
    "    number_df[number_df[\"distractor_number\"] == \"singular\"][\"murdoch_contribution_all\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1480dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11075391,  0.52858996],\n",
       "       [ 0.05287185,  0.0493324 ],\n",
       "       [ 0.04832102, -0.02005999],\n",
       "       [-0.13502298, -0.10924449],\n",
       "       [ 0.1682476 ,  0.07372447],\n",
       "       [ 0.23354782,  1.0271769 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_contributions.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "816f1c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15032065,  0.19854659],\n",
       "       [ 0.00806151,  0.09250602],\n",
       "       [ 0.08498269, -0.15722825],\n",
       "       [-0.05861461, -0.19921213],\n",
       "       [-0.03069262,  0.41683728],\n",
       "       [ 0.01059884,  0.76912606]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_contributions.mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1699e5b",
   "metadata": {},
   "source": [
    "### CD w/ Shapley bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cdac1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_model.shapley_include_bias = True\n",
    "decomposed_model.generalized = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52717305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/2305937806.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"sentence_tokens\"][0],\n",
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/2305937806.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"correct_token\"][0],\n",
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_87779/2305937806.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x[\"wrong_token\"][0]\n"
     ]
    }
   ],
   "source": [
    "number_df[\"murdoch_contribution_all\"] = number_df.apply(\n",
    "    lambda x: get_jumulet_contribution(\n",
    "        x[\"sentence_tokens\"][0],\n",
    "        x[\"correct_token\"][0],\n",
    "        x[\"wrong_token\"][0]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0c3a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# singular subject, plural distractor\n",
    "sp_contributions = np.stack(\n",
    "    number_df[number_df[\"distractor_number\"] == \"plural\"][\"murdoch_contribution_all\"]\n",
    "    )\n",
    "\n",
    "# singular subject\n",
    "ps_contributions = np.stack(\n",
    "    number_df[number_df[\"distractor_number\"] == \"singular\"][\"murdoch_contribution_all\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e649d3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2558565 , 0.08301068],\n",
       "       [0.18312702, 0.20157708],\n",
       "       [0.19482486, 0.09758325],\n",
       "       [0.18275078, 0.02052693],\n",
       "       [0.19952725, 0.18607761],\n",
       "       [0.10038027, 0.3066745 ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_contributions.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05367717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06104723, -7.9162555 ],\n",
       "       [ 0.06252284,  0.37209895],\n",
       "       [-0.61590654,  0.8521732 ],\n",
       "       [ 0.0444878 ,  0.31143522],\n",
       "       [ 0.09013457,  0.62945056],\n",
       "       [ 0.04749724,  0.31624857]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_contributions.mean(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaphor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
