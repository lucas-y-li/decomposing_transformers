{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers \n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import scipy.stats\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from decompose_bert import BertForMaskedLMDecomposed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = pd.read_csv(\"all_VERBs.csv\")[\"WORD\"]\n",
    "verb_ids = []\n",
    "\n",
    "for verb in verbs.iloc:\n",
    "    i = tokenizer.encode(\" \" + verb, add_special_tokens=False)\n",
    "    if (len(i) == 1):\n",
    "        verb_ids.append(i[0])\n",
    "\n",
    "verb_ids = torch.Tensor(verb_ids).to(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lq/kd3wh6952vg99n5ppf4srz9w0000gn/T/ipykernel_48599/2445828250.py:42: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  number_df = number_df.dropna().reset_index(drop=True).drop(columns=\"token\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject_number</th>\n",
       "      <th>distractor_number</th>\n",
       "      <th>sentence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">verb</th>\n",
       "      <th>verb_tokens</th>\n",
       "      <th>sentence_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correctness</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>wrong</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cats[MASK]</td>\n",
       "      <td>engages</td>\n",
       "      <td>engage</td>\n",
       "      <td>[24255, 8526]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 8870, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>604</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the cats[MASK]</td>\n",
       "      <td>remembers</td>\n",
       "      <td>remember</td>\n",
       "      <td>[17749, 3342]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 8870, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>605</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the chairs[MASK]</td>\n",
       "      <td>observes</td>\n",
       "      <td>observe</td>\n",
       "      <td>[24451, 11949]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 8397, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the dogs[MASK]</td>\n",
       "      <td>encourages</td>\n",
       "      <td>encourage</td>\n",
       "      <td>[16171, 8627]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 6077, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>608</td>\n",
       "      <td>singular</td>\n",
       "      <td>plural</td>\n",
       "      <td>The athlete behind the trucks[MASK]</td>\n",
       "      <td>avoids</td>\n",
       "      <td>avoid</td>\n",
       "      <td>[26777, 4468]</td>\n",
       "      <td>[101, 1996, 8258, 2369, 1996, 9322, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1793</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the cat[MASK]</td>\n",
       "      <td>avoid</td>\n",
       "      <td>avoids</td>\n",
       "      <td>[4468, 26777]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 4937, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1795</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the dog[MASK]</td>\n",
       "      <td>engage</td>\n",
       "      <td>engages</td>\n",
       "      <td>[8526, 24255]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 3899, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1797</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window[MASK]</td>\n",
       "      <td>avoid</td>\n",
       "      <td>avoids</td>\n",
       "      <td>[4468, 26777]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1799</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window[MASK]</td>\n",
       "      <td>engage</td>\n",
       "      <td>engages</td>\n",
       "      <td>[8526, 24255]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1800</td>\n",
       "      <td>plural</td>\n",
       "      <td>singular</td>\n",
       "      <td>The women near the window[MASK]</td>\n",
       "      <td>understand</td>\n",
       "      <td>understands</td>\n",
       "      <td>[3305, 19821]</td>\n",
       "      <td>[101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>561 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id subject_number distractor_number  \\\n",
       "correctness                                          \n",
       "0             603       singular            plural   \n",
       "1             604       singular            plural   \n",
       "2             605       singular            plural   \n",
       "3             606       singular            plural   \n",
       "4             608       singular            plural   \n",
       "..            ...            ...               ...   \n",
       "556          1793         plural          singular   \n",
       "557          1795         plural          singular   \n",
       "558          1797         plural          singular   \n",
       "559          1799         plural          singular   \n",
       "560          1800         plural          singular   \n",
       "\n",
       "                                        sentence         verb                \\\n",
       "correctness                                           correct         wrong   \n",
       "0              The athlete behind the cats[MASK]      engages        engage   \n",
       "1              The athlete behind the cats[MASK]    remembers      remember   \n",
       "2            The athlete behind the chairs[MASK]     observes       observe   \n",
       "3              The athlete behind the dogs[MASK]   encourages     encourage   \n",
       "4            The athlete behind the trucks[MASK]       avoids         avoid   \n",
       "..                                           ...          ...           ...   \n",
       "556                 The women near the cat[MASK]        avoid        avoids   \n",
       "557                 The women near the dog[MASK]       engage       engages   \n",
       "558              The women near the window[MASK]        avoid        avoids   \n",
       "559              The women near the window[MASK]       engage       engages   \n",
       "560              The women near the window[MASK]   understand   understands   \n",
       "\n",
       "                verb_tokens                                    sentence_tokens  \n",
       "correctness                                                                     \n",
       "0             [24255, 8526]  [101, 1996, 8258, 2369, 1996, 8870, 103, 102, ...  \n",
       "1             [17749, 3342]  [101, 1996, 8258, 2369, 1996, 8870, 103, 102, ...  \n",
       "2            [24451, 11949]  [101, 1996, 8258, 2369, 1996, 8397, 103, 102, ...  \n",
       "3             [16171, 8627]  [101, 1996, 8258, 2369, 1996, 6077, 103, 102, ...  \n",
       "4             [26777, 4468]  [101, 1996, 8258, 2369, 1996, 9322, 103, 102, ...  \n",
       "..                      ...                                                ...  \n",
       "556           [4468, 26777]  [101, 1996, 2308, 2379, 1996, 4937, 103, 102, ...  \n",
       "557           [8526, 24255]  [101, 1996, 2308, 2379, 1996, 3899, 103, 102, ...  \n",
       "558           [4468, 26777]  [101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...  \n",
       "559           [8526, 24255]  [101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...  \n",
       "560           [3305, 19821]  [101, 1996, 2308, 2379, 1996, 3332, 103, 102, ...  \n",
       "\n",
       "[561 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_df = pd.read_csv(\"nounpp.tsv\", delimiter=\"\\t\")\n",
    "max_length = 16\n",
    "\n",
    "number_df[\"id\"] = number_df[\"id\"].apply(lambda x: int(x[2:]))\n",
    "\n",
    "# only keep sentences that are plural/singular or singular/plural (distractor has different number)\n",
    "number_df[\"subject_distractor_number\"] = number_df[\"subject_distractor_number\"].apply(\n",
    "    lambda x: x if x == \"singular_plural\" or x == \"plural_singular\" else np.nan\n",
    ")\n",
    "number_df.dropna(inplace=True)\n",
    "\n",
    "number_df[\"subject_number\"] = number_df[\"subject_distractor_number\"].apply(lambda x: x.split(\"_\")[0])\n",
    "number_df[\"distractor_number\"] = number_df[\"subject_distractor_number\"].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "number_df[\"verb\"] = number_df[\"sentence\"].apply(lambda x: \" \" + x.split(\" \")[-1])\n",
    "number_df[\"sentence\"] = number_df[\"sentence\"].apply(lambda x: \" \".join(x.split(\" \")[:-1]) + \"[MASK]\")\n",
    "\n",
    "number_df = number_df.drop(\n",
    "    columns=[\"subject_distractor_number\"]\n",
    "    ).pivot(index=[\"id\", \"subject_number\", \"distractor_number\", \"sentence\"], columns=[\"correctness\"], values=[\"verb\"]).reset_index()\n",
    "\n",
    "\n",
    "def get_token(correct_token, wrong_token):\n",
    "    # drop if longer than 1 token\n",
    "    if len(correct_token) > 3 or len(wrong_token) > 3:\n",
    "        return np.nan\n",
    "\n",
    "    return [correct_token[1], wrong_token[1]]\n",
    "\n",
    "number_df[(\"token\", \"correct\")] = tokenizer(number_df[(\"verb\", \"correct\")].to_list())[\"input_ids\"]\n",
    "number_df[(\"token\", \"wrong\")] = tokenizer(number_df[(\"verb\", \"wrong\")].to_list())[\"input_ids\"]\n",
    "\n",
    "number_df[(\"verb_tokens\")] = number_df.apply(\n",
    "    lambda x: get_token(x[(\"token\", \"correct\")], x[(\"token\", \"wrong\")]),\n",
    "    axis=1)\n",
    "\n",
    "number_df[(\"sentence_tokens\")] = tokenizer(number_df[(\"sentence\", \"\")].to_list(),\n",
    "                                           max_length=max_length,\n",
    "                                           padding=\"max_length\")[\"input_ids\"]\n",
    "\n",
    "# drop duplicates\n",
    "number_df = number_df.dropna().reset_index(drop=True).drop(columns=\"token\")\n",
    "number_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1996, 8258, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 8258, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2879, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 7500, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 7500, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 7500, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2269, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2767, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2767, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2611, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 4845, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5160, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2158, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2158, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2388, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2388, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 3220, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 3220, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 3836, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 4470, 2369, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 6778, 3875, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2450, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 2450, 2379, 1996, 4624, 2015, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2369, 1996, 2482, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2369, 1996, 4937, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2369, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2369, 1996, 3332, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 3875, 1996, 7997, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 3875, 1996, 4624, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 3875, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 3875, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 2795, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3392, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3392, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 4744, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 5916, 2015, 2379, 1996, 3332, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2369, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2369, 1996, 3899, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 7997, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 2482, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 4744, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 3875, 1996, 3332, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 3242, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 4624, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 4624, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 4624, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n",
      "[101, 1996, 10533, 2015, 2379, 1996, 3392, 103, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_component_masks(sentence_tokens):\n",
    "    # prep = [639, 13276, 583] # beside, near, behind\n",
    "    prep = tokenizer.encode(' behind beside near')[1:-1]\n",
    "\n",
    "    # always starts with 0, 133\n",
    "    i = 2\n",
    "    init_i = 0\n",
    "    The_i = 1\n",
    "    subj_i = []\n",
    "\n",
    "    while sentence_tokens[i] not in prep:\n",
    "        subj_i.append(i)\n",
    "        i += 1\n",
    "        \n",
    "    prep_i = i\n",
    "    the_i = i + 1\n",
    "    distractor_id = []\n",
    "    i += 2\n",
    "\n",
    "    while sentence_tokens[i] != tokenizer.mask_token_id:\n",
    "        distractor_id.append(i)\n",
    "        i += 1\n",
    "    \n",
    "    mask_i = i\n",
    "    \n",
    "    # place each one in a separate array \n",
    "    component_masks = np.zeros((6, max_length))\n",
    "    for n, component in enumerate([\n",
    "        init_i, The_i, subj_i, prep_i, the_i, distractor_id\n",
    "    ]):\n",
    "        component_masks[n, component] = 1\n",
    "    \n",
    "    if mask_i != 6:\n",
    "        print(sentence_tokens)\n",
    "        print(component_masks)\n",
    "        print(mask_i)\n",
    "        print()\n",
    "    return component_masks, mask_i\n",
    "\n",
    "\n",
    "number_df[\"beta_mask\"] = number_df[\"sentence_tokens\"].apply(make_component_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pos_neg_contributions(logits):\n",
    "    \"\"\"\n",
    "    shape: (num_contributions + 1 (bias), num_classes)\n",
    "    \"\"\"\n",
    "   # put negative \"positive\" contributions in the negative category\n",
    "    # put negative \"negative\" contributions in the positive category\n",
    "    positive_mask = (logits > 0).astype(int)\n",
    "\n",
    "    positive_logits = logits[..., 1] * positive_mask[..., 1]\n",
    "    positive_logits -= logits[..., 0] * (1 - positive_mask[..., 0])\n",
    "\n",
    "    negative_logits = logits[..., 0] * positive_mask[..., 0]\n",
    "    negative_logits -= logits[..., 1] * (1 - positive_mask[..., 1])\n",
    "\n",
    "    binary_logits = np.stack(\n",
    "        [negative_logits, positive_logits], axis=-2\n",
    "    )\n",
    "    # print(binary_logits.shape)\n",
    "    return binary_logits\n",
    "\n",
    "\n",
    "def get_proportion_contribution(n, decomposed_model):\n",
    "    beta_masks = torch.tensor(number_df[\"beta_mask\"][n][0])\n",
    "    verb_tokens = torch.tensor(number_df[\"verb_tokens\"][n])\n",
    "    inputs = torch.tensor(number_df[\"sentence_tokens\"][n]).unsqueeze(0)\n",
    "    mask_i = number_df[\"beta_mask\"][n][1]\n",
    "\n",
    "    contribution_logits = torch.zeros((6, 2))\n",
    "    # print(tokenizer.convert_ids_to_tokens(verb_tokens))\n",
    "\n",
    "    for i, mask in enumerate(beta_masks):\n",
    "        beta_mask = torch.stack([mask, 1 - mask]).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            result = decomposed_model(input_ids=inputs,\n",
    "                            beta_mask=beta_mask)\n",
    "        # result = result[0, 0, mask_i, :]\n",
    "\n",
    "        # # normalize across all verbs\n",
    "        # result = result - result[verb_ids].mean()\n",
    "        # result = result / result[verb_ids].std()\n",
    "\n",
    "        # contribution = result[verb_tokens]\n",
    "        # contribution_logits[i, :] = contribution\n",
    "\n",
    "        result = result[:, 0, mask_i, :]\n",
    "        result = result - result[:, verb_ids].mean(1, keepdim=True)\n",
    "        result = result / result[:, verb_ids].std(1, keepdim=True)\n",
    "\n",
    "        correct, wrong = split_pos_neg_contributions(result[:, verb_tokens].numpy())\n",
    "        # beta_z_t / z_t\n",
    "        correct = correct[0] / (correct.sum() + 1e-10)\n",
    "        wrong = wrong[0] / (wrong.sum() + 1e-10)\n",
    "        \n",
    "        contribution_logits[i, 0] = correct\n",
    "        contribution_logits[i, 1] = wrong\n",
    "\n",
    "    return contribution_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultiBertGunjanPatrick/multiberts-seed-0\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "\n",
      "MultiBertGunjanPatrick/multiberts-seed-1\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "\n",
      "MultiBertGunjanPatrick/multiberts-seed-2\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "\n",
      "MultiBertGunjanPatrick/multiberts-seed-3\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "\n",
      "MultiBertGunjanPatrick/multiberts-seed-4\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "\n",
      "MultiBertGunjanPatrick/multiberts-seed-5\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "\n",
      "MultiBertGunjanPatrick/multiberts-seed-6\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "\n",
      "MultiBertGunjanPatrick/multiberts-seed-7\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "\n",
      "MultiBertGunjanPatrick/multiberts-seed-8\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     contribution_logits_sp\u001b[38;5;241m.\u001b[39mappend(get_proportion_contribution(i, decomposed_model))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     contribution_logits_ps\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_proportion_contribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecomposed_model\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mget_proportion_contribution\u001b[0;34m(n, decomposed_model)\u001b[0m\n\u001b[1;32m     32\u001b[0m beta_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([mask, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 35\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdecomposed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbeta_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# result = result[0, 0, mask_i, :]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# # normalize across all verbs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# contribution = result[verb_tokens]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# contribution_logits[i, :] = contribution\u001b[39;00m\n\u001b[1;32m     46\u001b[0m result \u001b[38;5;241m=\u001b[39m result[:, \u001b[38;5;241m0\u001b[39m, mask_i, :]\n",
      "File \u001b[0;32m~/Desktop/code/work/cpsyd-summer-summarization/nouns_modifiers/transformers/decompose/number_agreement/../decompose_bert.py:660\u001b[0m, in \u001b[0;36mBertForMaskedLMDecomposed.__call__\u001b[0;34m(self, input_ids, beta_mask, token_type_ids, attention_mask)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, beta_mask, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 660\u001b[0m     sequence_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_decomposed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermutations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_contributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_contributions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m     prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_decomposed(\n\u001b[1;32m    666\u001b[0m         sequence_output, perms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermutations)\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m    669\u001b[0m         \u001b[38;5;66;03m# verify correctness\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/code/work/cpsyd-summer-summarization/nouns_modifiers/transformers/decompose/number_agreement/../decompose_bert.py:463\u001b[0m, in \u001b[0;36mBertModelDecomposed.__call__\u001b[0;34m(self, input_ids, beta_mask, perms, token_type_ids, attention_mask, num_contributions)\u001b[0m\n\u001b[1;32m    458\u001b[0m embedding_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_decomposed(\n\u001b[1;32m    459\u001b[0m     input_ids, beta_mask, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m    460\u001b[0m     num_contributions\u001b[38;5;241m=\u001b[39mnum_contributions)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# encoding layers\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_decomposed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m pooled_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler_decomposed(encoded_layers, perms)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (encoded_layers, pooled_outputs)\n",
      "File \u001b[0;32m~/Desktop/code/work/cpsyd-summer-summarization/nouns_modifiers/transformers/decompose/number_agreement/../decompose_bert.py:332\u001b[0m, in \u001b[0;36mBertEncoderDecomposed.__call__\u001b[0;34m(self, xs, attention_mask, perms)\u001b[0m\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(xs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_decomposed):\n\u001b[0;32m--> 332\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;66;03m# verify correctness\u001b[39;00m\n\u001b[1;32m    336\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m layer_module\u001b[38;5;241m.\u001b[39mlayer(\n\u001b[1;32m    337\u001b[0m             hidden_states, attention_mask)\n",
      "File \u001b[0;32m~/Desktop/code/work/cpsyd-summer-summarization/nouns_modifiers/transformers/decompose/number_agreement/../decompose_bert.py:302\u001b[0m, in \u001b[0;36mBertLayerDecomposed.__call__\u001b[0;34m(self, xs, attention_mask, perms)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs, attention_mask, perms):\n\u001b[1;32m    300\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(xs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 302\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_decomposed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     intermediate_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_decomposed(\n\u001b[1;32m    305\u001b[0m         attention_outputs, perms\u001b[38;5;241m=\u001b[39mperms)\n\u001b[1;32m    306\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_decomposed(\n\u001b[1;32m    307\u001b[0m         intermediate_outputs, attention_outputs)\n",
      "File \u001b[0;32m~/Desktop/code/work/cpsyd-summer-summarization/nouns_modifiers/transformers/decompose/number_agreement/../decompose_bert.py:206\u001b[0m, in \u001b[0;36mBertAttentionDecomposed.__call__\u001b[0;34m(self, xs, attention_mask)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs, attention_mask):\n\u001b[1;32m    203\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_decomposed(\n\u001b[1;32m    204\u001b[0m         xs, attention_mask)\n\u001b[0;32m--> 206\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_decomposed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m    211\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(xs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/code/work/cpsyd-summer-summarization/nouns_modifiers/transformers/decompose/number_agreement/../decompose_bert.py:179\u001b[0m, in \u001b[0;36mBertSelfOutputDecomposed.__call__\u001b[0;34m(self, xs_hidden, xs_input)\u001b[0m\n\u001b[1;32m    175\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(xs_input, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    177\u001b[0m xs_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_decomposed(xs_hidden)\n\u001b[0;32m--> 179\u001b[0m xs_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm_decomposed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs_hidden\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxs_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# verify correctness\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer(hidden_states, input_tensor)\n",
      "File \u001b[0;32m~/Desktop/code/work/cpsyd-summer-summarization/nouns_modifiers/transformers/decompose/number_agreement/../decompose_util.py:32\u001b[0m, in \u001b[0;36mLayerNormDecomposed.__call__\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     30\u001b[0m u_components \u001b[38;5;241m=\u001b[39m x_components\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m x_components \u001b[38;5;241m=\u001b[39m (x_components \u001b[38;5;241m-\u001b[39m u_components) \u001b[38;5;241m/\u001b[39m div\n\u001b[0;32m---> 32\u001b[0m y_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m x_components\n\u001b[1;32m     34\u001b[0m u_bias \u001b[38;5;241m=\u001b[39m x_bias\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m x_bias \u001b[38;5;241m=\u001b[39m (x_bias \u001b[38;5;241m-\u001b[39m u_bias) \u001b[38;5;241m/\u001b[39m div\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sps = []\n",
    "pss = []\n",
    "\n",
    "with open(\"multibert_results.txt\", 'a') as f:\n",
    "    for n in range(0, 25):\n",
    "        model_name = f\"MultiBertGunjanPatrick/multiberts-seed-{n}\"\n",
    "        if n == 11:\n",
    "            # https://huggingface.co/MultiBertGunjanPatrick/multiberts-seed-11 \n",
    "            # latest commit does not contain the model files for some reason\n",
    "            # use older commit instead\n",
    "            decomposed_model = BertForMaskedLMDecomposed.from_pretrained(\n",
    "                model_name, revision=\"a6b3771a09d37a29953fdf28b146b8da34e29f0e\",\n",
    "                generalized=True,\n",
    "                num_contributions=2,\n",
    "                debug=False\n",
    "            )\n",
    "        else:\n",
    "            decomposed_model = BertForMaskedLMDecomposed.from_pretrained(\n",
    "                model_name,\n",
    "                generalized=True,\n",
    "                num_contributions=2,\n",
    "                debug=False\n",
    "            )\n",
    "        print()\n",
    "        print(model_name)\n",
    "\n",
    "        contribution_logits_sp = []\n",
    "        contribution_logits_ps = []\n",
    "\n",
    "        for i in range(len(number_df)):\n",
    "            if number_df[\"subject_number\"][i] == \"singular\":\n",
    "                contribution_logits_sp.append(get_proportion_contribution(i, decomposed_model))\n",
    "            else:\n",
    "                contribution_logits_ps.append(get_proportion_contribution(i, decomposed_model))\n",
    "            if i % 20 == 0:\n",
    "                print(i)\n",
    "\n",
    "        contribution_logits_sp = torch.stack(contribution_logits_sp)\n",
    "        contribution_logits_ps = torch.stack(contribution_logits_ps)\n",
    "\n",
    "        sp = contribution_logits_sp.mean(0).numpy()\n",
    "        ps = contribution_logits_ps.mean(0).numpy()\n",
    "        sps.append(sp)\n",
    "        pss.append(ps)\n",
    "\n",
    "        f.write(model_name + \"\\n\")\n",
    "        f.write(np.array2string(sp) + \"\\n\")\n",
    "        f.write(np.array2string(ps) + \"\\n\\n\")\n",
    "        f.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(pss, open(\"pss_save_.pkl\", \"wb\"))\n",
    "pkl.dump(sps, open(\"sps_save_.pkl\", \"wb\"))\n",
    "# pss = pkl.load(open(\"pss_save.pkl\", \"rb\"))\n",
    "# sps = pkl.load(open(\"sps_save.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 6, 2)\n",
      "(25, 6, 2)\n"
     ]
    }
   ],
   "source": [
    "pss = np.stack(pss)\n",
    "sps = np.stack(sps)\n",
    "\n",
    "print(pss.shape)\n",
    "print(sps.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13,  0.01],\n",
       "       [-0.26, -0.2 ],\n",
       "       [ 1.09, -1.56],\n",
       "       [ 0.03, -0.19],\n",
       "       [-0.43, -0.22],\n",
       "       [-0.41, -0.25]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pss.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22, -0.22],\n",
       "       [ 0.79, -0.87],\n",
       "       [ 0.85, -0.15],\n",
       "       [ 0.37, -0.33],\n",
       "       [ 0.05, -0.46],\n",
       "       [-1.26,  0.19]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sps.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.34, -0.15],\n",
       "        [-0.43, -0.39],\n",
       "        [ 0.92, -1.91],\n",
       "        [-0.12, -0.38],\n",
       "        [-0.6 , -0.39],\n",
       "        [-0.51, -0.4 ]]),\n",
       " array([[ 0.08,  0.17],\n",
       "        [-0.09, -0.01],\n",
       "        [ 1.25, -1.2 ],\n",
       "        [ 0.18,  0.  ],\n",
       "        [-0.27, -0.05],\n",
       "        [-0.31, -0.11]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "scipy.stats.t.interval(0.95, pss.shape[0] - 1, loc=pss.mean(0), scale=scipy.stats.sem(pss, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.05, -0.46],\n",
       "        [ 0.57, -0.98],\n",
       "        [ 0.7 , -0.29],\n",
       "        [ 0.18, -0.48],\n",
       "        [-0.16, -0.64],\n",
       "        [-1.47,  0.03]]),\n",
       " array([[ 0.38,  0.02],\n",
       "        [ 1.01, -0.76],\n",
       "        [ 1.  , -0.  ],\n",
       "        [ 0.55, -0.18],\n",
       "        [ 0.26, -0.27],\n",
       "        [-1.04,  0.35]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.t.interval(0.95, sps.shape[0] - 1, loc=sps.mean(0), scale=scipy.stats.sem(sps, axis=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaphor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
